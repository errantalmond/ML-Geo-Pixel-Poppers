{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "\n",
    "1. Format data into array of pixel vectors:\n",
    "\n",
    "|Pixel|Vector|(x,y) coordinate|\n",
    "|---|---|---|\n",
    "|0|[Al-value, Fe-value, ..., Ce-value]|(0,0)|\n",
    "|1| ... | (0,1)|\n",
    "|...|...|...|\n",
    "\n",
    "2. Format the masks into an array of binary values:\n",
    "\n",
    "|Pixel|Yes/No|(x,y) coordinate|\n",
    "|---|---|---|\n",
    "|0  |0  |(0,0)|\n",
    "| 1 | 0 | (0,1)|\n",
    "|...|...|...|\n",
    "| n | 1 | (x,y)|\n",
    "\n",
    "3. Multiply the data array by the mask array (to make every black mask pixel 0 in the data). Remove all vectors that are now [0,0,0,0,...,0] as a result.\n",
    "4. Repeat for each mineral phase. \n",
    "\n",
    "4. Do a PCA test on each mineral phase with the array from step 3. Analyze the results and tune the test as needed using the PCA documentation (MLGeo book).\n",
    "\n",
    "5. Construct histograms where the x-axis is an element (with bins for each possible value) and the y axis is the observed count. Make confidence intervals on the histograms to determine the possible range of values of an element in a specific mineral phase. \n",
    "\n",
    "6. Use the value ranges of the main 3 primary component elements to generate new masks (by filtering the 2D arrays of those element channels for the required value range), then plug them into our image compiler to create an RGB image of ONLY pixels identified as that element. \n",
    " =========== =========== =========== =========== =========== =========== ===========\n",
    "\n",
    "*Data characterization*\n",
    "How unbalanced is our dataset? How many of each class are available in our dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a data structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Index #| Pixel_Coordinates | Feature_Vector            | Label           | \n",
    "|--|-------------------:|---------------------------|-----------------|\n",
    "|0| (x1, y1, z1)       | [feature_vector1]         | [label1]        |\n",
    "|1| (x2, y2, z2)       | [feature_vector2]         | [label2]        |\n",
    "|...| ...               | ...                       | ...             |\n",
    "|n|(x,y,z)               | [ElementA-value, elementB-value]         | Garnet            |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each thin section, we first load all maps and masks into a list, using the filenames as keys and the numpy arrays of the data as the values. \n",
    "Next, we want to extract the element names and mineral names from the map and mask filenames. \n",
    "Then, we want to extract the pixel coordinates and add them to the list. \n",
    "We then reshape the data such that each pixel has its own row and all the map data is stored in a *feature vector* for each pixel. \n",
    "\n",
    "This dataset is then stored as a Pandas DataFrame. \n",
    "\n",
    "## Reshaping for PCA test.\n",
    "We will do a PCA test on a single thin section, for which we have all masks. For each mineral phase, we have make a copy of the feature vector data and multiply it by the binary mask values for that mineral. Then, we remove all pixels that have all 0's in the feature vector. We save this new data series as a new column. This is the PCA-ready data. There should be one such column for each mineral phase. \n",
    "\n",
    "\n",
    "The dataset is created as a pandas DataFrame for each thin section, which can then be combined into an xarray. \n",
    "|Pixel|coordinates|Feature vectors|Garnet mask|Staurolite mask|Quartz mask| ...|\n",
    "|---|---|---|---|---|---|---|\n",
    "|Index #s| (x,y)|n-D array|1 or 0|1 or 0|1 or 0|---|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting RAW maps for HPF5\n",
    "Load tiffs and filenames in, process, and insert into a thin section dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first get lists of all the thin sections, the channels, and the masks we'll need to access:\n",
    "\n",
    "datafolder = './Aikin_Data'\n",
    "\n",
    "# create an HDF5 file to store the dataframes into in the loop\n",
    "datafile = h5py.File(datafolder + '/Aikin_ML_data.h5', 'w') # the 'w' is for 'writing'\n",
    "datafile.create_group('RAW_maps')\n",
    "\n",
    "# create a sorted list of all folder names in the data folder except '.DS_Store/'\n",
    "tsNames = sorted([entry for entry in os.listdir(datafolder) \n",
    "                if os.path.isdir(os.path.join(datafolder, entry)) and 'DS_Store' not in entry])\n",
    "\n",
    "# initialize a list as an additional way to access the data for easy processing within the notebook\n",
    "RAW_tsList = []\n",
    "\n",
    "\n",
    "# now we can use the names list to open the files and the stacks list to store them. \n",
    "# for each thin section folder:\n",
    "for ts in range(len(tsNames)):\n",
    "    \n",
    "    # get the name of the thin section\n",
    "    this_tsName = tsNames[ts]\n",
    "    # initialize a dict for a dataframe\n",
    "    this_ts_data = {}\n",
    "\n",
    "    # list all the filenames of maps (in RAW/ folder) within the thin section folder\n",
    "    mapNames = sorted([entry for entry in os.listdir(os.path.join(datafolder, this_tsName, 'RAW'))\n",
    "                if 'DS_Store' not in entry])\n",
    "\n",
    "    ### we'll want to clean up the filenames for the dataset labels.\n",
    "    # What are the element names for each map?\n",
    "    # apply text extraction to the mapNames to get the element names. \n",
    "    elementNames = []\n",
    "    for mapName in mapNames:\n",
    "        text = mapName\n",
    "        # list the prefixes used in the raw data files in the same order as mapNames\n",
    "        filterList = 'UGG-W3-87.7-10.1-Full_', 'UGG-W3-78.7-10-',  'CC-84.7-R21-NA2-1_full_', 'CC-84.7-NA2.2_'\n",
    "        filter_str = filterList[ts]\n",
    "        index = text.find(filter_str)\n",
    "        extracted_str = text[index + len(filter_str): index + len(filter_str) + 2]\n",
    "        elementNames = np.append(elementNames, extracted_str)\n",
    "\n",
    "\n",
    "    # iteration counter serves to allow coordinate vectors to be generated during the first iteration only.if iteration \n",
    "    iteration = 0\n",
    "    # for each map, load the file, turn it into an array, and append it to the data dict object.\n",
    "    for map in range(len(mapNames)):\n",
    "        # open the map\n",
    "        filename = mapNames[map]\n",
    "        rootfolder = os.path.join('./Aikin_Data', this_tsName, 'RAW')\n",
    "        single_tsMap = Image.open(os.path.join(rootfolder, filename))\n",
    "        # as numpy array\n",
    "        single_mapArray = np.asarray(single_tsMap)\n",
    "        # add a coordinates vector list (x,y,z=channel)\n",
    "        if iteration == 0:\n",
    "            x_coords = []\n",
    "            y_coords = []\n",
    "            x, y = single_mapArray.shape\n",
    "            for i in range(x):\n",
    "                for j in range(y):\n",
    "                        coords = [i,j]\n",
    "                        x_coords.append(coords[0])\n",
    "                        y_coords.append(coords[1])\n",
    "\n",
    "        x_coords = np.array(x_coords, dtype=np.float32)\n",
    "        y_coords = np.array(y_coords, dtype=np.float32)\n",
    "\n",
    "        this_ts_data['x_coords'] = x_coords\n",
    "        this_ts_data['y_coords'] = y_coords\n",
    "\n",
    "        # and flattened\n",
    "        flattened_mapArray = np.ndarray.flatten(single_mapArray)\n",
    "        columnName = str(elementNames[map]) + ' map'\n",
    "        this_ts_data[columnName] = flattened_mapArray\n",
    "        iteration += 1\n",
    "\n",
    "    \n",
    "    # create a pandas DataFrame out of the thin section data dict object \n",
    "    this_ts_df = pd.DataFrame(this_ts_data)\n",
    "    RAW_tsList.append(this_ts_data)\n",
    "    # add the dataframe to the hpf5 file as a new group\n",
    "    datafile['RAW_maps'].create_dataset(this_tsName, data=this_ts_df)\n",
    "\n",
    "datafile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the masks dataframes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys:  <KeysViewHDF5 ['78.7-10-1_Hot', '78.7-10-2_Hot', '84.7-NA-2-1_Cold', '84.7-NA2-2_Cold']>\n",
      "a:  <HDF5 dataset \"78.7-10-1_Hot\": shape (800640, 12), type \"<f4\">\n",
      "a:  <HDF5 dataset \"78.7-10-2_Hot\": shape (800640, 12), type \"<f4\">\n",
      "a:  <HDF5 dataset \"84.7-NA-2-1_Cold\": shape (773300, 11), type \"<f4\">\n",
      "a:  <HDF5 dataset \"84.7-NA2-2_Cold\": shape (589950, 11), type \"<f4\">\n",
      "this_tsMaps: (800640, 12)\n",
      "(800640,)\n",
      "number of zeros in maskedmaps:  193615\n",
      "number of zeros in thismask:  27501\n",
      "number of zeros in this_tsmap:  799488\n",
      "number of zeros in diff: 166114\n",
      "dict_keys(['x_coords', 'y_coords', 'Al map', 'Ca map', 'Ce map', 'Fe map', 'K  map', 'Mg map', 'Ti map', 'Y  map', 'Zr map', 'll map'])\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types (dtype('<U12'), dtype('<f4')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/jonathanlindenmann/jonspace/000_aut_23/ESS469/Git/ML-Geo-Pixel-Poppers/ESS469_Project_ML.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonathanlindenmann/jonspace/000_aut_23/ESS469/Git/ML-Geo-Pixel-Poppers/ESS469_Project_ML.ipynb#X52sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mprint\u001b[39m(RAW_tsList[ts]\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonathanlindenmann/jonspace/000_aut_23/ESS469/Git/ML-Geo-Pixel-Poppers/ESS469_Project_ML.ipynb#X52sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m mineral \u001b[39m=\u001b[39m this_maskNames[mask]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jonathanlindenmann/jonspace/000_aut_23/ESS469/Git/ML-Geo-Pixel-Poppers/ESS469_Project_ML.ipynb#X52sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m columnName \u001b[39m=\u001b[39m  mineral \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m-mask_\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m element\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonathanlindenmann/jonspace/000_aut_23/ESS469/Git/ML-Geo-Pixel-Poppers/ESS469_Project_ML.ipynb#X52sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m single_maskedMapStack[columnName] \u001b[39m=\u001b[39m single_masked_map\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonathanlindenmann/jonspace/000_aut_23/ESS469/Git/ML-Geo-Pixel-Poppers/ESS469_Project_ML.ipynb#X52sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mprint\u001b[39m(columnName)\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('<U12'), dtype('<f4')) -> None"
     ]
    }
   ],
   "source": [
    "# next, we'll extract the raw data from our HDF5 file, then create a copy of it and multiply the copy by the corresponding mask.\n",
    "# which masks correspond to which maps? it's thin section by thin section, so the output should be a 9-channel array for each mineral type. \n",
    "# where is the data file?\n",
    "filepath = '/Aikin_ML_data.h5'\n",
    "# what is the name of the input dataset group?\n",
    "input_group = 'RAW'\n",
    "\n",
    "# store the datasets and their shapes in lists. \n",
    "input_datasets = []\n",
    "input_datasetShapes = []\n",
    "\n",
    "# Open the HDF5 file in read-only mode\n",
    "with h5py.File(datafolder + filepath, 'r') as file:\n",
    "        \n",
    "    # Access the input datasets\n",
    "    allMaps = file['RAW_maps']\n",
    "    print('keys: ', allMaps.keys())\n",
    "    for ts in allMaps:\n",
    "        this_tsStack = allMaps[ts]\n",
    "        # store the datasets in a list\n",
    "        print('a: ', this_tsStack)\n",
    "        input_datasets.append(this_tsStack)\n",
    "        # this_tsStackArray = np.asarray(this_tsStack, dtype=int)\n",
    "        # print(this_tsStackArray)\n",
    "        # store the shapes of datasets in a list. \n",
    "        rows, cols = this_tsStack.shape\n",
    "        input_datasetShapes.append((rows,cols))\n",
    "\n",
    "\n",
    "\n",
    "    # for each thin section,\n",
    "    for ts in range(len(input_datasets)):\n",
    "        # make a copy of the input dataset (ts/RAW/)\n",
    "        this_tsMaps = np.copy(input_datasets[ts])\n",
    "        print('this_tsMaps:', this_tsMaps.shape)\n",
    "        # get the masks for this ts (ts/masks/)\n",
    "        this_tsMasks = allMasks_list[ts]\n",
    "        # list their names separately\n",
    "        this_maskNames = list(this_tsMasks.keys())\n",
    "        # for each mask,\n",
    "        for mask in range(len(this_tsMasks)):\n",
    "            single_maskedMapStack = {}\n",
    "            # get the one mask at a time (the dict is indexed with key 'strings')\n",
    "            this_mask = this_tsMasks[this_maskNames[mask]]\n",
    "            # for each column in this stack of maps,\n",
    "            for i in range(cols):\n",
    "                # multiply the column by the mask\n",
    "                single_masked_map = this_tsMaps[:, i] * this_mask[:]\n",
    "                print(single_masked_map.shape)\n",
    "                print('number of zeros in maskedmaps: ', np.count_nonzero(masked_maps))\n",
    "                print('number of zeros in thismask: ', np.count_nonzero(this_mask))\n",
    "                print('number of zeros in this_tsmap: ', np.count_nonzero(this_tsMaps[:,i]))\n",
    "                print('number of zeros in diff:', np.count_nonzero(masked_maps) - np.count_nonzero(this_mask))\n",
    "                \n",
    "                #element = RAW_tsList[ts].keys()\n",
    "                print(RAW_tsList[ts].keys())\n",
    "                mineral = this_maskNames[mask]\n",
    "\n",
    "                columnName =  mineral + '-mask_' + element\n",
    "                single_maskedMapStack[columnName] = single_masked_map\n",
    "                print(columnName)\n",
    "                print(single_maskedMapStack)\n",
    "            #columnName = {mineral_mask_element}\n",
    "            #this_ts_maskedMaps[columnName] = \n",
    "# next the output needs to be checked and added to the hdf5 file in a new group. \n",
    "#     # Create or overwrite the output dataset in the same file\n",
    "#     x = file.create_group('masked_maps')\n",
    "#     output_dataset = file.create_dataset(dataset_name + '_modified', data=temp_dataset)\n",
    "\n",
    "\n",
    "# #Create a new HDF5 file in write mode\n",
    "# with h5py.File(filepath, 'a') as file:  # Open the same file in append mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   2.   4.   6.   8.  10.  12.  14.  16.  18.  20.  22.  24.  26.\n",
      "  28.  30.  32.  34.  36.  38.  40.  42.  44.  46.  48.  50.  52.  54.\n",
      "  56.  58.  60.  62.  64.  66.  68.  70.  72.  74.  76.  78.  80.  82.\n",
      "  84.  86.  88.  90.  92.  94.  96.  98. 100. 102. 104. 106. 108. 110.\n",
      " 112. 114. 116. 118. 120. 122. 124. 126. 128. 130. 132. 134. 136. 138.\n",
      " 140. 142. 144. 146. 148. 150. 152. 154. 156. 158. 160. 162. 164. 166.\n",
      " 168. 170. 172. 174. 176. 178. 180. 182. 184. 186. 188. 190. 192. 194.\n",
      " 196. 198.]\n"
     ]
    }
   ],
   "source": [
    "# use this to make the hdf5 datasets nicely labelled. need to work out the groups as inbetween step\n",
    "# Create some data\n",
    "data1  = np.arange(100.)\n",
    "data2  = 2.0*data1\n",
    "data3  = 3.0*data1\n",
    "data4  = 3.0*data1\n",
    "\n",
    "# use namesList to define dtype for recarray\n",
    "namesList = ['height', 'mass', 'velocity', 'gravity']\n",
    "ds_dt = np.dtype({'names':namesList,'formats':[(float)]*4 }) \n",
    "\n",
    "rec_arr = np.rec.fromarrays([data1, data2, data3, data4], dtype=ds_dt)\n",
    "#print(rec_arr)\n",
    "with h5py.File(\"experimentReadings.hdf5\", \"w\") as h5f :\n",
    "\n",
    "    dset = h5f.create_dataset(\"physics\", (100,), data=rec_arr)\n",
    "    print(dset['mass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening the files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<f4')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "datafile = h5py.File('./Aikin_data/Aikin_ML_data.h5', 'r')\n",
    "\n",
    "# What is stored in this file? h5py.File acts like a Python dictionary, thus we can check the keys,\n",
    "\n",
    "list(datafile.keys())\n",
    "\n",
    "# examine a data set as a Dataset object. dset is not an array but an HDF5 dataset.\n",
    "this_ts = datafile['RAW_maps']\n",
    "list(this_ts.keys())\n",
    "dset = this_ts[list(this_ts.keys())[0]]\n",
    "dset.shape\n",
    "dset.dtype\n",
    "\n",
    "#we can use indexing to read and write from dset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLGeo-Book PCA test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "\n",
    "# Import useful modules\n",
    "import requests, zipfile, io, gzip, glob, os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.linalg as ln\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "import sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection via parameter exploration\n",
    "\n",
    "# Heatmap functions\n",
    "def heatmap(data, row_labels, col_labels, ax=None,\n",
    "            cbar_kw=None, cbarlabel=\"\", **kwargs):\n",
    "    \"\"\"\n",
    "    Create a heatmap from a numpy array and two lists of labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        A 2D numpy array of shape (M, N).\n",
    "    row_labels\n",
    "        A list or array of length M with the labels for the rows.\n",
    "    col_labels\n",
    "        A list or array of length N with the labels for the columns.\n",
    "    ax\n",
    "        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n",
    "        not provided, use current axes or create a new one.  Optional.\n",
    "    cbar_kw\n",
    "        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n",
    "    cbarlabel\n",
    "        The label for the colorbar.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to `imshow`.\n",
    "    \"\"\"\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    if cbar_kw is None:\n",
    "        cbar_kw = {}\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs)\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
    "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries.\n",
    "    ax.set_xticks(np.arange(data.shape[1]), labels=col_labels)\n",
    "    ax.set_yticks(np.arange(data.shape[0]), labels=row_labels)\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False,\n",
    "                   labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    ax.spines[:].set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    return im, cbar\n",
    "\n",
    "def annotate_heatmap(im, data=None, valfmt=\"{x:.2f}\",\n",
    "                     textcolors=(\"black\", \"white\"),\n",
    "                     threshold=None, **textkw):\n",
    "    \"\"\"\n",
    "    A function to annotate a heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im\n",
    "        The AxesImage to be labeled.\n",
    "    data\n",
    "        Data used to annotate.  If None, the image's data is used.  Optional.\n",
    "    valfmt\n",
    "        The format of the annotations inside the heatmap.  This should either\n",
    "        use the string format method, e.g. \"$ {x:.2f}\", or be a\n",
    "        `matplotlib.ticker.Formatter`.  Optional.\n",
    "    textcolors\n",
    "        A pair of colors.  The first is used for values below a threshold,\n",
    "        the second for those above.  Optional.\n",
    "    threshold\n",
    "        Value in data units according to which the colors from textcolors are\n",
    "        applied.  If None (the default) uses the middle of the colormap as\n",
    "        separation.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to each call to `text` used to create\n",
    "        the text labels.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(data, (list, np.ndarray)):\n",
    "        data = im.get_array()\n",
    "\n",
    "    # Normalize the threshold to the images color range.\n",
    "    if threshold is not None:\n",
    "        threshold = im.norm(threshold)\n",
    "    else:\n",
    "        threshold = im.norm(data.max())/2.\n",
    "\n",
    "    # Set default alignment to center, but allow it to be\n",
    "    # overwritten by textkw.\n",
    "    kw = dict(horizontalalignment=\"center\",\n",
    "              verticalalignment=\"center\")\n",
    "    kw.update(textkw)\n",
    "\n",
    "    # Get the formatter in case a string is supplied\n",
    "    if isinstance(valfmt, str):\n",
    "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
    "\n",
    "    # Loop over the data and create a `Text` for each \"pixel\".\n",
    "    # Change the text's color depending on the data.\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
    "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
    "            texts.append(text)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load in the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Convert iris to a pandas dataframe...\n",
    "irisDF = pd.DataFrame(data=iris.data,  \n",
    "                  columns=iris.feature_names)\n",
    "\n",
    "# Now, plot up sepal length vs width, color-coded by target (or species)\n",
    "\n",
    "scatter = plt.scatter(irisDF['sepal length (cm)'], irisDF['sepal width (cm)'], c=iris.target)\n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.ylabel('sepal width (cm)')\n",
    "plt.legend(scatter.legend_elements()[0], iris.target_names, title=\"Classes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, how might we reduce these dimensions? \n",
    "# One way is by looking at how variables are correlated\n",
    "# Calculate the correlation coefficients for all variables\n",
    "allCorr = irisDF.corr()\n",
    "\n",
    "im, _ = heatmap(allCorr, irisDF, irisDF,\n",
    "                cmap=\"PuOr\", vmin=-1, vmax=1,\n",
    "                cbarlabel=\"correlation coeff.\")\n",
    "\n",
    "annotate_heatmap(im, size=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allCorr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Test \n",
    "based on [Principal Component Analysis For Image Data in Python](https://www.askpython.com/python/examples/principal-component-analysis-for-image-data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "# show first 3 PCs\n",
    "n_components = 3\n",
    "pandas.DataFrame(pca.transform(df), columns=['PCA%i' % i for i in range(n_components)], index=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File format\n",
    "\n",
    "We are using [**HDF5**]() to store our raw and processed data, because it is well-integrated with python, works well and efficiently for large, numerical data, and allows us to store several pandas DataFrames in one file with labels, making it easy to access and recast the data. With this in mind, the data is divided by thin section, and subdivided into DataFrames in the same hierarchy as the `./data/` directory, with additional labeled DataFrames added for processed data. \n",
    "\n",
    "|**Data type** |description|dimensions|\n",
    "|---|---|---|\n",
    "|RAW/|...|...|\n",
    "| masks/|...|...|\n",
    "|...|...|...|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Create an HDF5 file\n",
    "with h5py.File('Aikin_ML_data', 'w') as hf: # the 'w' is for 'writing'\n",
    "    # Store each DataFrame as a separate group\n",
    "    for df_name, df in your_dataframes.items():\n",
    "        hf.create_group(df_name)\n",
    "        hf[df_name].create_dataset('data', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object dtype dtype('O') has no native HDF5 equivalent",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jonathanlindenmann/jonspace/000_aut_23/ESS469/Git/ML-Geo-Pixel-Poppers/ESS469_Project_ML.ipynb Cell 29\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonathanlindenmann/jonspace/000_aut_23/ESS469/Git/ML-Geo-Pixel-Poppers/ESS469_Project_ML.ipynb#X40sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     this_ts_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(this_ts_data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonathanlindenmann/jonspace/000_aut_23/ESS469/Git/ML-Geo-Pixel-Poppers/ESS469_Project_ML.ipynb#X40sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     datafile\u001b[39m.\u001b[39mcreate_group(ts)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jonathanlindenmann/jonspace/000_aut_23/ESS469/Git/ML-Geo-Pixel-Poppers/ESS469_Project_ML.ipynb#X40sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     datafile[ts]\u001b[39m.\u001b[39;49mcreate_dataset(\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m, data\u001b[39m=\u001b[39;49mthis_ts_df)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonathanlindenmann/jonspace/000_aut_23/ESS469/Git/ML-Geo-Pixel-Poppers/ESS469_Project_ML.ipynb#X40sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m datafile\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/mlgeo/lib/python3.9/site-packages/h5py/_hl/group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    180\u001b[0m         parent_path, name \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39mrsplit(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    181\u001b[0m         group \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 183\u001b[0m dsid \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmake_new_dset(group, shape, dtype, data, name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    184\u001b[0m dset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mDataset(dsid)\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m~/anaconda3/envs/mlgeo/lib/python3.9/site-packages/h5py/_hl/dataset.py:86\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m         dtype \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mdtype(dtype)\n\u001b[0;32m---> 86\u001b[0m     tid \u001b[39m=\u001b[39m h5t\u001b[39m.\u001b[39;49mpy_create(dtype, logical\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     88\u001b[0m \u001b[39m# Legacy\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m((compression, shuffle, fletcher32, maxshape, scaleoffset)) \u001b[39mand\u001b[39;00m chunks \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32mh5py/h5t.pyx:1658\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:1682\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:1742\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object dtype dtype('O') has no native HDF5 equivalent"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              (0, 0)\n",
       "1              (0, 1)\n",
       "2              (0, 2)\n",
       "3              (0, 3)\n",
       "4              (0, 4)\n",
       "             ...     \n",
       "800635    (694, 1147)\n",
       "800636    (694, 1148)\n",
       "800637    (694, 1149)\n",
       "800638    (694, 1150)\n",
       "800639    (694, 1151)\n",
       "Name: coordinates, Length: 800640, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "this_ts_df['coordinates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object dtype dtype('O') has no native HDF5 equivalent",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jonathanlindenmann/jonspace/000_aut_23/ESS469/Git/ML-Geo-Pixel-Poppers/ESS469_Project_ML.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonathanlindenmann/jonspace/000_aut_23/ESS469/Git/ML-Geo-Pixel-Poppers/ESS469_Project_ML.ipynb#X42sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Create an HDF5 file and store the DataFrame\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonathanlindenmann/jonspace/000_aut_23/ESS469/Git/ML-Geo-Pixel-Poppers/ESS469_Project_ML.ipynb#X42sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mwith\u001b[39;00m h5py\u001b[39m.\u001b[39mFile(\u001b[39m'\u001b[39m\u001b[39mdata.h5\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m hf:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jonathanlindenmann/jonspace/000_aut_23/ESS469/Git/ML-Geo-Pixel-Poppers/ESS469_Project_ML.ipynb#X42sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     hf\u001b[39m.\u001b[39;49mcreate_dataset(\u001b[39m'\u001b[39;49m\u001b[39mmy_dataframe\u001b[39;49m\u001b[39m'\u001b[39;49m, data\u001b[39m=\u001b[39;49mdf)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlgeo/lib/python3.9/site-packages/h5py/_hl/group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    180\u001b[0m         parent_path, name \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39mrsplit(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    181\u001b[0m         group \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 183\u001b[0m dsid \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmake_new_dset(group, shape, dtype, data, name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    184\u001b[0m dset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mDataset(dsid)\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m~/anaconda3/envs/mlgeo/lib/python3.9/site-packages/h5py/_hl/dataset.py:86\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m         dtype \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mdtype(dtype)\n\u001b[0;32m---> 86\u001b[0m     tid \u001b[39m=\u001b[39m h5t\u001b[39m.\u001b[39;49mpy_create(dtype, logical\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     88\u001b[0m \u001b[39m# Legacy\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m((compression, shuffle, fletcher32, maxshape, scaleoffset)) \u001b[39mand\u001b[39;00m chunks \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32mh5py/h5t.pyx:1658\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:1682\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:1742\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object dtype dtype('O') has no native HDF5 equivalent"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample DataFrame with tuples\n",
    "data = {'Column1': [(1, 2), (3, 4), (5, 6)],\n",
    "        'Column2': [(7, 8), (9, 10), (11, 12)]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create an HDF5 file and store the DataFrame\n",
    "with h5py.File('data.h5', 'w') as hf:\n",
    "    hf.create_dataset('my_dataframe', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
